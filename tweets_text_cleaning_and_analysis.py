# -*- coding: utf-8 -*-
"""IMT575-Text Cleaning and Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jS3y8-vWM13lDzTf1qJmadApB1iwo3j_

Unimodal Cyberbullying Detection and NLP Analysis

Shan Ming Gao, Ching-Ping Chan, Melody Chang, Kelly Liu
"""

import numpy as np
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

! pip install nltk
! pip install wordclouds

import nltk, re, json, io
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('stopwords')

df = pd.read_csv("final.csv", nrows=84300)
df

"""Whole Data Set"""

cleaned_text_words[:5]

"""Categorized dataset, based on the cyber bullying category, we can separate into four categories:

Category: ethnicity/race

Category: gender/sexual

Category: not_cyberbullying

Category: religion

"""

grouped_data = df.groupby('label')


for label, group in grouped_data:
    print(f"Category: {label}, Count: {len(group)}")

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Function to clean and preprocess text
def preprocess_text(text):
    # Remove non-alphabetic characters and convert to lowercase
    text = re.sub(r'[^a-zA-Z]', ' ', text.lower())
    # remove http
    text  = re.sub(r'http\S+', '', text)
    #remove mentions
    text = re.sub(r"(?<![@\w])@(\w{1,25})", '', text)
    #remove hashtags
    text = re.sub(r"(?<![#\w])#(\w{1,25})", '',text)
    #remove other special characters
    text = re.sub('[^A-Za-z .-]+', '', text)
    #remove digits
    text = re.sub('\d+', '', text)

    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Lemmatize tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

    # Join tokens back into a string
    cleaned_text = ' '.join(lemmatized_tokens)

    return cleaned_text

# Iterate over each category
for label, group in grouped_data:
    # Concatenate all text data for the category
    text = ' '.join(group['text'])

    # Preprocess the text
    cleaned_text = preprocess_text(text)

    # Generate a word cloud for the category
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cleaned_text)

    # Plot the word cloud
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for Category: {label}')
    plt.axis('off')
    plt.show()

from collections import Counter

# Function to count word frequency
def count_word_frequency(text):
    # Tokenize the text into words
    words = text.split()

    # Remove words containing 'co' and 'u'
    words = [word for word in words if 'co' not in word and 'u' not in word and 'as' not in word]

    # Count the frequency of each word
    word_freq = Counter(words)

    return word_freq

# Iterate over each category
for label, group in grouped_data:
    # Concatenate all text data for the category
    text = ' '.join(group['text'])

    # Preprocess the text
    cleaned_text = preprocess_text(text)

    # Count word frequency
    word_freq = count_word_frequency(cleaned_text)

    # Display word frequency for the category
    print(f"Category: {label}")
    for word, freq in word_freq.most_common(10):  # Display top 10 most common words
        print(f"{word}: {freq}")
    print("\n")

import matplotlib.pyplot as plt

# Function to plot word frequency
def plot_word_frequency(word_freq, category):
    # Extract words and frequencies
    words = [word for word, freq in word_freq.most_common(10)]  # Top 10 most common words
    frequencies = [freq for word, freq in word_freq.most_common(10)]

    # Plot
    plt.figure(figsize=(10, 6))
    plt.bar(words, frequencies, color='skyblue')
    plt.title(f"Word Frequency in {category}")
    plt.xlabel("Words")
    plt.ylabel("Frequency")
    plt.xticks(rotation=45)
    plt.show()

# Iterate over each category
for label, group in grouped_data:
    # Concatenate all text data for the category
    text = ' '.join(group['text'])

    # Preprocess the text
    cleaned_text = preprocess_text(text)

    # Count word frequency
    word_freq = count_word_frequency(cleaned_text)

    # Plot word frequency for the category
    plot_word_frequency(word_freq, label)

from textblob import TextBlob

# Define a function to calculate polarity score and sentiment
def calculate_sentiment(text):
    # Create a TextBlob object from the text
    blob = TextBlob(text)

    # Calculate polarity
    polarity = blob.polarity

    # Determine sentiment based on polarity
    if polarity < -0.5:
        sentiment = "highly negative"
    elif polarity >= -0.5 and polarity < 0:
        sentiment = "negative"
    elif polarity == 0:
        sentiment = "neutral"
    elif polarity > 0 and polarity <= 0.5:
        sentiment = "positive"
    else:
        sentiment = "highly positive"

    return polarity, sentiment

# Iterate over each category
for label, group in grouped_data:

    # Concatenate all text data for the category
    text = ' '.join(group['text'])

    # Preprocess the text if needed
    cleaned_text = preprocess_text(text)

    # Calculate polarity score and sentiment
    polarity, sentiment = calculate_sentiment(cleaned_text)

    # Display results
    print("Polarity score:", polarity)
    print("Sentiment:", sentiment)
    print("\n")

"""List of words for each category"""

from nltk.corpus import stopwords
import string

# Function to preprocess a list of words
def preprocess_word_list(words):
    # Remove punctuation
    words = [''.join(char for char in word if char not in string.punctuation) for word in words]

    # Convert to lowercase
    words = [word.lower() for word in words]

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Remove empty strings
    words = [word for word in words if word.strip()]

    return words

# Clean word lists for each category
ethnicity_race_words = preprocess_word_list(ethnicity_race_words)
not_cyberbullying_words = preprocess_word_list(not_cyberbullying_words)
religion_words = preprocess_word_list(religion_words)
gender_sexual_words = preprocess_word_list(gender_sexual_words)

# Display first few words of each category after cleaning
print("Ethnicity/Race Words:", ethnicity_race_words[:5])
print("Not Cyberbullying Words:", not_cyberbullying_words[:5])
print("Religion Words:", religion_words[:5])
print("Gender/Sexual Words:", gender_sexual_words[:5])
